{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP-Bigrams-Final.ipynb","provenance":[],"collapsed_sections":["5HOqIgfuZ3pN","TP5I0qlzVkzB"],"mount_file_id":"1wfPzJTP2T3R3hw7Wz9twGBTSiJthq0vG","authorship_tag":"ABX9TyNhhAt2+aW5w3CQuyl4eXj8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5HOqIgfuZ3pN"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"TP5I0qlzVkzB"},"source":["## Functions used in this section"]},{"cell_type":"code","metadata":{"id":"pTy_cE5_VhoG"},"source":["from sklearn.model_selection import train_test_split\r\n","\r\n","# Generate a python dictionary of (character bigram : frequency)\r\n","# pairs, can be used to construct multi-lingual dictionary.\r\n","# Parameters:\r\n","#   text        - Language text to analyse.\r\n","#   bigramFreq  - An input dictionary that can be built upon.\r\n","def makeFreqDict(text, bigramFreq):\r\n","    for i in range(1, len(text)):\r\n","        bigram = str(text[i-1] + text[i])\r\n","        bigramFreq[bigram] = 0\r\n","    return bigramFreq\r\n","\r\n","\r\n","# Count the frequency of character bigrams and increment the\r\n","# appropriate attribute of the dictionary.\r\n","# Parameters:\r\n","#   text          - Language text to count the frequencies of.\r\n","#   templateDict  - The multi-lingual dictionary created via  makeFreqDict.\r\n","def fillFreqDict(text, templateDict):\r\n","    bigramFreq = templateDict.copy()\r\n","\r\n","    for i in range(1, len(text)):\r\n","        bigram = str(text[i-1] + text[i])\r\n","        if bigram in bigramFreq.keys():\r\n","            bigramFreq[bigram] = bigramFreq[bigram] + 1\r\n","    return list(bigramFreq.values())\r\n","\r\n","\r\n","\r\n","# Split the data by a number of characters and then count\r\n","# the frequency of bigrams within each split.\r\n","# Parameters:\r\n","#   data      - Data of large volume that will be split.\r\n","#   freqDict  - Multi-lingual template dictionary to count frequencies using.\r\n","#   label     - Label detailing the origin language of the data.\r\n","#   splitSize - The size of data samples to be taken from data.\r\n","def itemizeData(data, freqDict, label, splitSize):\r\n","  dataSplits = []\r\n","  items = []\r\n","\r\n","  for i in range(0, len(data)-splitSize, splitSize):\r\n","    dataSplits.append(data[i:i+splitSize])\r\n","\r\n","  for i in range(0, len(dataSplits)):\r\n","    freq = fillFreqDict(dataSplits[i], freqDict)\r\n","    freq.append(label)\r\n","    items.append(list(freq))\r\n","  return items\r\n","\r\n","\r\n","\r\n","# Split the data into distinct training and testing sets.\r\n","# Parameters:\r\n","#   data - Language data to be split.\r\n","def ttSplit(data):\r\n","  train, test = train_test_split(dataSplits, test_size=0.3, random_state=42, shuffle=True)\r\n","  return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRQ5AyVYU-25"},"source":["## Reading the data"]},{"cell_type":"code","metadata":{"id":"yjINa-RBQ8Ms"},"source":["import re\r\n","import numpy as np\r\n","\r\n","regexCond = \"<[\\w]+>|</[\\w]+>|<+[\\w]+ \\w+=[^<^>]+>|\\n\"      # Regex string that allows finding xml tags in language corpuses.\r\n","\r\n","file = open(\"english.txt\", \"r\")                             # Reading in the english corpus.\r\n","engData = file.read()\r\n","file.close()\r\n","\r\n","file = open(\"czech.txt\", \"r\", encoding=\"utf-16\")            # Reading in the czech corpus.\r\n","czechData = file.read()\r\n","file.close()\r\n","\r\n","file = open(\"combined.txt\", \"r\", encoding=\"utf-16\")         # Reading in the igbo corpus.\r\n","igboData = file.read()\r\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cc1PEgITVHjb"},"source":["## Using regex to remove XML"]},{"cell_type":"code","metadata":{"id":"ebWzp-lXVF8P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614874263225,"user_tz":0,"elapsed":5017,"user":{"displayName":"Samuel Hardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiYqzd_teQbmVmnyJ6NqUVxV0OQk3NYkQ5uP2a=s64","userId":"12115027515166882012"}},"outputId":"7c9301cb-2d0a-4f97-e65a-7b98302014fd"},"source":["engData = re.sub(regexCond, \"\", engData)                      # Using regex string to remove xml tags from english corpus.\r\n","czechData = re.sub(regexCond, \"\", czechData)                  # Using regex string to remove xml tags from czech corpus.\r\n","igboData = re.sub(regexCond, \"\", igboData)                    # Using regex string to remove xml tags from igbo corpus.\r\n","\r\n","print(len(engData) + len(czechData) + len(igboData))      "],"execution_count":null,"outputs":[{"output_type":"stream","text":["13695123\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aqrINjGEVQ5T"},"source":["## Find all potential bigrams"]},{"cell_type":"code","metadata":{"id":"KiqVbJtbVPGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614874278096,"user_tz":0,"elapsed":19879,"user":{"displayName":"Samuel Hardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiYqzd_teQbmVmnyJ6NqUVxV0OQk3NYkQ5uP2a=s64","userId":"12115027515166882012"}},"outputId":"5ec481ee-05bc-47af-cd95-9dc7afab52cc"},"source":["engFreq = makeFreqDict(engData, {})                         # Start making a multi-lingual bigram character dictionary.\r\n","engCzechFreq = makeFreqDict(czechData, engFreq)             # Add the bigrams found in czech that don't exist in english.\r\n","fullFreq = makeFreqDict(igboData, engCzechFreq)             # Add the bigrams found in igbo that don't exist in english or czech.\r\n","\r\n","engTrain, engTest = ttSplit(engData)                        # Split english corpus into training and testing sets.\r\n","czechTrain, czechTest = ttSplit(czechData)                  # Split czech corpus into training and testing sets.\r\n","igboTrain, igboTest = ttSplit(igboData)                     # Split igbo corpus into training and testing sets.\r\n","\r\n","engTrain = fillFreqDict(engTrain, fullFreq)                 # Creating the english training data.\r\n","engTrain.append(0)                                          # Adding the label to the training data (not used - just for sanity).\r\n","czechTrain = fillFreqDict(czechTrain, fullFreq)             # Creating the czech training data.\r\n","czechTrain.append(1)                                        # Adding the label to the training data (not used - just for sanity).\r\n","igboTrain = fillFreqDict(igboTrain, fullFreq)               # Creating the igbo training data.\r\n","igboTrain.append(2)                                         # Adding the label to the training data (not used - just for sanity).\r\n","\r\n","engTest = itemizeData(engTest, fullFreq, 0, 500)            # Count the frequencies of bigrams in each X character split using english test data.\r\n","czechTest = itemizeData(czechTest, fullFreq, 1, 500)        # Count the frequencies of bigrams in each X character split using czech test data.\r\n","igboTest = itemizeData(igboTest, fullFreq, 2, 500)          # Count the frequencies of bigrams in each X character split using igbo test data.\r\n","\r\n","trainData = np.vstack([engTrain, czechTrain])               # Create a training set where the first row is english corpus frequencies and the\r\n","trainData = np.vstack([trainData, igboTrain])               # second and third rows are czech and igbo frequencies respectively.\r\n","\r\n","trainLabels = trainData[:, -1]                              # Harvest the training data labels from the training data.\r\n","trainLabels = trainLabels.reshape(trainLabels.shape[0])     # Reshape the training data labels into a single row format.\r\n","trainData = np.delete(trainData, -1, 1)                     # Delete the training data labels from the training data.\r\n","\r\n","testData = np.vstack([engTest, czechTest])                  # Create a testing set where the first row is english corpus frequencies and the\r\n","testData = np.vstack([testData, igboTest])                  # second and third rows are czech and igbo frequencies respectively.\r\n","\r\n","testLabels = testData[:,-1]                                 # Harvest the testing data labels from the test data.\r\n","testLabels = testLabels.reshape(testLabels.shape[0])        # Reshape the testing data labels into a single row format.\r\n","testData = np.delete(testData, -1, 1)                       # Delete the testing data labels from the test data.\r\n","\r\n","print(testData.shape)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(5477, 6950)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j3e4PqfxpaQG"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"OdIfpdslVtmG"},"source":["## Creating the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSvfT0KgV44Z","executionInfo":{"status":"ok","timestamp":1614874283117,"user_tz":0,"elapsed":1129,"user":{"displayName":"Samuel Hardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiYqzd_teQbmVmnyJ6NqUVxV0OQk3NYkQ5uP2a=s64","userId":"12115027515166882012"}},"outputId":"7ae74e6b-72e0-49bc-c4f4-02e976fba5b9"},"source":["import numpy as np\r\n","from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\r\n","from sklearn.metrics.pairwise import cosine_similarity\r\n","from scipy.stats import mode \r\n","\r\n","\r\n","# Compare two data sets using cosine similarity to\r\n","# determine the origin language of the first data set.\r\n","# Parameters:\r\n","#   test  - The test data to be classified.\r\n","#   train - The training data to be used for comparison. \r\n","def similarityTest(test, train):\r\n","  similarity = cosine_similarity(test, train)               # Create a similarity matrix where each row is the similarity\r\n","  predictions = []                                          # between one test sample and each training sample.\r\n","\r\n","  for row in similarity:\r\n","    predictions.append(np.argmax(row, axis=0))              # Find which training sample is closest to the test sample and\r\n","  return predictions                                        # make that training samples label the test samples label.\r\n","\r\n","\r\n","predictions = similarityTest(testData, trainData)           # Getting the prediction for each test data sample.\r\n","acc = accuracy_score(testLabels, predictions)               # Calculating the accuracy using the predictions\r\n","print(acc)                                                  # and the known correct labels.\r\n","f1 = f1_score(testLabels, predictions, average=\"macro\")     # Calculating the f1 score using the predictions\r\n","print(f1)                                                   # and the known correct labels."],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n","1.0\n"],"name":"stdout"}]}]}